{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"About Me","text":"<p>I am a Data and Analytics Engineer with experience building and maintaining data infrastructure and data models.</p> <p>Passionnant about building infrastructure using linux, docker and gitlab pipelines but also in the cloud mostly using AWS and terraform. Data modelling using dbt in order to create and expand on the existing data world. I am very strict to myself when it comes to following standards being that coding or writing documentation.</p> <p>Feel free to check my  GitHub to see what project I am working on.</p> <p>Area of Interest</p> <ul> <li> <p> Data Analytics</p> <p>Telling a story and creating a compelling presantation with data</p> </li> <li> <p> Data Modeling</p> <p>Design and implement data models that support business requirements</p> </li> <li> <p> Deployment Automation DevOps</p> <p>Streamline workflow using CI/CD, infrastructure as code (IaC) and pipelines</p> </li> <li> <p> Cloud Computing</p> <p>Creating infrastrucutre in the cloud using terraform to process and gather data</p> </li> <li> <p> Machine Learning</p> <p>Create models that learn from data, identifying patterns and making decisions based on that information</p> </li> <li> <p> Internet of Things IoT</p> <p>Connects devices to enabling data exchange and automation for smarter solutions</p> </li> </ul> <p>Tech and Tools</p> <ul> <li> Python</li> <li> Docker</li> <li> AWS</li> <li> Linux</li> <li> Terraform</li> <li> git</li> <li> <p> dbt</p> </li> <li> <p> Apache Hadoop</p> </li> <li> Apache Spark</li> <li> Apache Hive</li> <li> <p> Apache Airflow</p> </li> <li> <p> bash</p> </li> <li> Go lang</li> <li> R language</li> <li></li> <li> vim</li> <li> K8s</li> <li> /  GitLab/GitHub</li> <li> Ansible</li> </ul>"},{"location":"blog/","title":"Blog","text":""},{"location":"cv/","title":"Index","text":""},{"location":"cv/#ditmar-halla","title":"Ditmar Halla","text":"<p> LinkedIn: linkedin.com/in/ditmar GitHub: github.com/ditmarhalla </p>"},{"location":"cv/#summary","title":"Summary","text":"<p>Data Engineer with years of experience creating integration data pipelines and data architecture to provide quality data. Focus in understanding the data and business requirements. Track record of deploying production quality code and process improvement.</p>"},{"location":"cv/#work-experience","title":"Work Experience","text":""},{"location":"cv/#data-and-analytics-engineer","title":"Data and Analytics Engineer","text":"<p>Haufe Group - Freiberg, DE (remote) July 2022  Ongoing </p> <p>Hired by the Data and Analytics team in order to support in the improvement of the data platform, with close to 25 Tb of data, more then 30 systems and 80 data pipelines. Python, dbt, docker, Linux, Terraform, AWS, Ansible, GitLab CI/CD</p> <ul> <li>Build and maintain data pipelines from different use-cases.</li> <li>Create and maintain architecture and systems documentation.</li> <li>Collaborate with Data Analysts to drive efficiencies for their work.</li> <li>Generate architecture recommendations and implemented them in production.</li> <li>Responsible for major and minor sections of the data model.</li> <li>Carry out full system data migration and platform migration.</li> </ul>"},{"location":"cv/#werkstudent-in-software-testing-and-automation","title":"Werkstudent in Software Testing and Automation","text":"<p>Mbition GmbH (Mercedes-Benz Software Hub) - Berlin, DE January 2022  June 2022 </p> <p>Hired by Mercedes-Benz software hub in the verification team to support in the development of a new framework in testing the latest released software build, using robotics and computer vision. Python, docker, Jenkins, Robot Framework, Linux</p> <ul> <li>Create low and high level API in python for both user and server side.</li> <li>Maintaining the hardware of the test equipment.</li> <li>Analyse Jenkins and robot framework test reports to keep the pipeline running without interruptions.</li> </ul>"},{"location":"cv/#research-assistant","title":"Research Assistant","text":"<p>Hochschule Mittweida - Mittweida, DE March 2021  December 2021 </p> <p>Approached by Numerical Mathematics Professor to join research in running simulation for calculating Thermo-electrohydrodynamic convection. Matlab, bash, linux</p> <ul> <li>Work on the High-Performance Computing (HPC) to run Computational Fluid Dynamics (CFD) simulations to visualize fluid flow.</li> <li>Create shell scripting to automate the running of the simulations.</li> <li>Analyse the results using statistical modeling with Matlab.</li> </ul>"},{"location":"projects/","title":"List of some of my projects","text":""},{"location":"projects/#machine-learning","title":"Machine Learning","text":""},{"location":"projects/#numerical-mathematics-matlab","title":"Numerical Mathematics (Matlab)","text":""},{"location":"projects/#sunspot-detection-python-cv2","title":"Sunspot Detection (Python + cv2 )","text":""},{"location":"projects/#calculating-the-correlation","title":"Calculating the correlation","text":""},{"location":"projects/#clasification-project-python-oop-numpy","title":"Clasification Project (Python OOP + NumPy)","text":""},{"location":"projects/classification/","title":"Clasification-Project","text":""},{"location":"projects/classification/#this-is-a-project-i-buid-from-scrach-i-was-only-proveded-the-two-text-files-of-trained-and-untrained-data","title":"This is a project I buid from scrach. I was only proveded the two text files of trained and untrained data.","text":"<p>Python project intended as an introduction into machine learning and classification (Part of the University Curuculum)</p> <ul> <li> <p>In this classification we have an array of data. Each array is composed by 7 numbers in the trained.txt dataset and by 6 numbers in the untrained.txt dataset. In the trained dataset we have 15 000 array and in the untrained one we have 1.4 million array. Using python we have to classify the untrained data using the trained data that we have and calculate the 7th value which is the classification of the array.</p> </li> <li> <p>In the first iteration I used the python integrated methods and also only using functions. To convert the file takes very little time, around 1.0 secons.</p> </li> <li>But when we try and classify the untraind vectors than we see the problem. It takes 47.9 seconds just to classify 1000 untrained vectors.</li> <li> <p>That is why we define a fuction that removes the same value from each vector and also check that the accuracy is still intact. This will make the program run faster as it has less calculations</p> </li> <li> <p>Then we implement the NumPy library and also object oriented programing This makes the program run much faster. So fast that we dont need to remove any dimanesions. But we see that loading all the array of 1.4 million takes a long time. Around 10 seconds. But that is not a problem since the calculation is so fast that it shows up as 1.5 seconds.</p> </li> </ul>"},{"location":"projects/ml/","title":"Machine Learning","text":"<p>GitHub</p> <p>The code for this project can be found here  Jupyter Notebook Part 1 Jupyter Notebook Part 2 </p>"},{"location":"projects/ml/#first-part-clustering-and-gmm-model","title":"First Part: Clustering and GMM model","text":"<p>In this part, we will determine a reasonable number of clusters inside a given data set, using K-means as the clustering algorithm, and two different performance evaluation method to decide on the optimal parameter k, the Elbow method, and Silhouette score. After that, we construct a Gaussian Mixture Model and fit the given data set.</p>"},{"location":"projects/ml/#elbow-method","title":"Elbow Method","text":"<p>The Elbow method which runs K-means clustering on the data set for a range of values for k (say from 1-10), plots the curve of the average score for the model depending on values for k. We will choose the optimal K at which the addition of clustering stops giving significant improvement of the score, where the \"elbow\" happens in the curve.The score metric applied for this method, amongst many, is the WCSS score, the sum of square distances from each point to its assigned center. Applying this score metric, a lower score indicates a tighter clustering scheme, and hence a better performing model.</p> <p>The metric used for the Elbow method is within-cluster sum-of-squares (WCSS). To calculate WCSS, we compute:</p> <p>\\(S_i = (\\displaystyle \\sum_{d=1}^D (x_i^d- \\mu_k^d))^\\frac{1}{2}\\), Euclidean distance from \\(x_i\\) to its cluster centroid \\(\\mu_k\\) \\(S_k = \\displaystyle \\frac{1}{N}\\sum_{i=1}^{N_k} S_i\\), the average within-cluster sum-of-squares for one cluster \\(k\\) WCSS = \\(\\displaystyle \\frac{1}{K}\\sum_{k=1}^{K} S_k\\), the average within-cluster sum-of-squares for all K clusters.  </p> <p>Essentially, WCSS measures the variability of the observations within each cluster. In general, a cluster that has a small sum of squares is more compact than a cluster that has a large sum of squares. Clusters that have higher values exhibit greater variability of the observations within the cluster. WCSS suffers from various drawbacks:</p> <p>WCSS makes the assumption that clusters are convex and isotropic, which is not always the case. It responds poorly to elongated clusters, or manifolds with irregular shapes.</p> <p>WCSS is not a normalized metric: we just know that lower values are better and zero is optimal. But in very high-dimensional spaces, Euclidean distances tend to become inflated. Running a dimensionality reduction algorithm such as Principal component analysis (PCA) prior to k-means clustering can alleviate this problem and speed up the computations.</p> <p></p> <p>After running dimensional reduction on the 10-dimensional data set down to only 2-dimensional data set, we plot the curve of WCSS against a range of k in (2, 14). From the illustration \\ref{Elbow}, the \"elbow\" happens at value k in range (6,8), outside of which any subsequent values of k fail to give much better clustering fit. We can argue for this case, the optimal k can be any value from 6,7,8.</p>"},{"location":"projects/ml/#silhouette-score","title":"Silhouette score","text":"<p>The second evaluation method is through the silhouette score, the normalized difference between distances of intra-cluster and nearest-cluster distance. Applying this score metric, We choose the optimal value where the silhouette score is maximized.</p> <p>The silhouette score is computed as the normalized difference between two distances for every point \\(x_i\\) in the data set. The first one records the distance between \\(x_i\\) to all members sharing its clusters. The second one measures the distance between \\(x_i\\) and the foreign nearest cluster. The mean difference across all data points gives us the silhouette score, such that, a cluster that has high silhouette score is more compact and separated to other clusters.</p> <p></p> <p>We plot the curve of WCSS against a range of k in (3, 12). From the illustration, the performance is maximized at value k = 8, with the second best at k = 7. Combine with the result from the Elbow method, we can confidently select k = 8 to be the optimal number of clusters for this data set.</p>"},{"location":"projects/ml/#gaussian-mixture-model","title":"Gaussian Mixture Model","text":"<p>A Gaussian mixture model is a probabilistic model that assumes all the data points are generated from a mixture of a finite number of Gaussian distributions with unknown parameters, specifically the mean and co-variance matrix. The Gaussian mixture model implements the expectation-maximization  algorithm for fitting mixture-of-Gaussian models. The algorithm alternates between two main steps. From the current Gaussian cluster characteristics, it assigns a weight variable to a data point \\(x_i\\), which corresponds to how likely \\(x_i\\) belongs to each Gaussian \\(k\\) (Expectation step). Then from the standpoint of the data points, it redefines every Gaussian by the new parameters based solely on the newly acquired weights.</p> <p>Let the data set X contains data point \\(x_i\\), \\(i = 1,2,\\dots ,n\\) of \\(d\\)-dimensional space. We assume a mixture of \\(K\\) finite Gaussian distributions, each Gaussian \\(k\\) is characterized by a set of mean and co-variance matrix (\\(\\mu_k, \\Sigma_k\\)). Each data point \\(x_i\\) admits a probability with respect to each Gaussian cluster \\(k\\), \\(N(\\mu_k, \\Sigma_k)\\). We also assume a prior probability \\(\\pi_k\\), how likely that Gaussian \\(k\\) is chosen. The Expectation-Maximization is performed as follows.</p> <ol> <li>Initialize the prior probability \\(\\pi_k\\), and the Gaussian \\(\\mu_k, \\Sigma_k\\), for every \\(k = 1, 2, .., K\\).</li> <li>Predict a weight for every pair data point \\(x_i\\) and Gaussian \\(k\\), for how likely that \\(x_i\\) belongs to this Gaussian \\(k\\). \\(\\gamma(i,k) = \\frac{N(\\mu_k, \\Sigma_k).\\pi_k}{\\sum_{j = 1}^K N(\\mu_k, Sigma_k).\\pi_j}\\)</li> <li>Using only the new weight \\(\\gamma(i,k)\\) as navigation for the K Gaussian clusters, redefine the Gaussian clusters by the new prior probability \\(\\pi_k\\), and new parameters \\((\\mu_k, \\Sigma_k)\\).</li> <li>Check for convergence, conclude the iterative process if the weights' difference between two iterations \\(\\epsilon\\) is smaller than some preset threshold.</li> </ol> <p></p> <p>Based on the previous results, the optimal number of clusters for this particular data set is 8 clusters. We reduce out data set to only 2-dimensional space, and apply a pre-built Gaussian Mixture Model.</p>"},{"location":"projects/ml/#second-part-classification","title":"Second Part: Classification","text":"<p>The K-nearest neighbors (KNN) algorithm is a type of supervised machine learning algorithms. We will explore the theory behind KNN algorithm, and address some problems that often occur in its implementation. Then, we will build a Python module that gives us a KNN classifer for a training data set, and also solve some of the problems discussed before. </p>"},{"location":"projects/ml/#algorithm","title":"Algorithm","text":"<p>Let \\((X,C)\\) be the training set of n data points, $X \\subset \\mathbb{R}^d $ and label set \\(C= \\{c_1, c_2, \\dots , c_m\\}\\) with the class labelling function \\(c: \\mathbb{R}^d \\longrightarrow C\\). Assume \\(c(x_i)\\) is the class label of a data point \\(x_i\\). The KNN algorithm uses  \\((X,C)\\) to identify the class \\(c(\\hat{x})\\) of the new data point \\(\\hat{x}\\) in three main steps. KNN algorithm\\</p> <ol> <li>calculates the distances \\(d(\\hat{x}, x_i)\\), \\(i = 1,2,...,n\\)</li> <li>sorts points by the distances: \\(d(\\hat{x}, x(1)) \\leq d(\\hat{x}, x(2)) \\leq \\dots \\leq d(\\hat{x}, x(n))\\)</li> <li>decides for class \\(c_i\\)} if: \\(i = \\argmax_{j=1,..,m} \\sum_{nb=1}^k \\sigma(c_j, c(x_{nb})), \\sigma(a,b)= \\begin{cases}    1 &amp;\\text{if  }  a=b\\\\    0 &amp;\\text{if  }  otherwise \\end{cases}\\) i.e. the winner-class is whichever class with the most appearances in K-nearest neighbors. An alternative decision-making method is choosing the best scored class, in which a neighbor's score is weighted by how close it is to the data point \\(\\hat{x}\\), as opposed to uniform weighted. In this case, the algorithm decides for class \\(c_i\\) if \\(i = \\argmax_{j=1,..,m} \\sum_{nb=1}^k \\sigma(c_j, c(x_{nb})).w_m, \\sigma(a,b) = \\begin{cases}     1 &amp; \\text{if } a=b\\\\     0 &amp; \\text{if } otherwise \\end{cases}\\) where \\(w_m =\\displaystyle \\frac{ \\frac{1}{d(x(nb),\\hat{x})}} {\\sum_{p=1}^k \\frac{1}{d(x(p),\\hat{x})}}\\) is the normalized distance from neighbor $m $ to new data point \\(x\\).</li> </ol>"},{"location":"projects/ml/#problems","title":"Problems","text":"<p>One of the most important remarks on the KNN algoritm is that a particular model of KNN classifer is built on a specific training data set. The problems that we discuss below are specific to this training data set, as are the methods that we address those problems.  This particular training data set \\((X,C)\\) has 637 data points, $X \\subset \\mathbb{R}^2 $,  \\(C= \\{c_1, c_2\\}\\).</p>"},{"location":"projects/ml/#parameter-selection","title":"Parameter Selection","text":"<p>Finding an optimal K parameter is perhaps the very first problem one might encounter while implementing the K-nearest Neighborhood Algorithm. We can address this problem by allowing the KNN classifier to perform cross validation on our training data set, with various K parameters. The model with the best performance with provide us the optimal K which is best suited for this particular training data set. There are indeed a few techniques of cross validation, however we will apply the n-fold cross validation technique, to evaluate KNN models powered by a range of K = 1,...\\ The steps are as follows:  </p> <ol> <li>Choose K value for the classifier.  </li> <li>The data set \\((X,C)\\) is partitioned into n sub-folds (subsets), $(X,C) = \\bigcup\\limits_{s=1}^{n} (X_s,C_s) $  </li> <li>For each \\(s\\), take the testing data $(X_{test},C_{test}) = (X_s,C_s) $, and training data $(X_{train},C_{train})= (X,C) \\setminus (X_{s},C_{s}) $.  \\(X_{test}\\) is to be classified by a K-NN classifier trained on the training data. The accuracy rate is chosen as the performance metric for this s, defined as: \\(E_s = \\frac{\\displaystyle \\sum_{j = 1}^{|X_{test}|}\\sigma(c_j, c(x_j))}{|X_{test}|}, \\quad \\sigma(c_j, c(x_j))=       \\begin{cases}       1,&amp; \\text{if } c_j= c(x_j)\\\\       0,              &amp; \\text{otherwise}       \\end{cases}\\) </li> <li>Evaluate for all \\(s\\)'s, and obtain average performance metric for the whole model at this particular K parameter, \\(E_K =\\displaystyle \\frac{1}{n}. \\displaystyle \\sum_{s=1}^n E_s\\) </li> <li>Compare the performance of K-NN classifer models in a range of K, and choose the best performing model.  </li> </ol> <p>The n-fold cross validation is suitable for our training data set, since with 637 data points of 2-dimensional space, the data set is large enough such that the selected training set can generally represent the entire data set, but not too large such that we risk the validation going for too long.  </p> <p>The listing results below compare average accuracy rates between classifier models varied by parameter K, and also varied by the decision-scheme.</p> Listing 1 , 10-fold cross-validation k =  1 , by uniform:  0.906 , by distance:  0.906 k =  2 , by uniform:  0.915 , by distance:  0.906 k =  3 , by uniform:  0.925 , by distance:  0.922 k =  4 , by uniform:  0.928 , by distance:  0.922 k =  5 , by uniform:  0.92 , by distance:  0.923 k =  6 , by uniform:  0.923 , by distance:  0.917 k =  7 , by uniform:  0.916 , by distance:  0.92 k =  8 , by uniform:  0.923 , by distance:  0.917 k =  9 , by uniform:  0.917 , by distance:  0.917 <p>Overall, performance is not substantially different between the two decision-making schemes for every pair models of the same parameter K. As for the parameter K selection, K \\(\\in\\) {3,4} generates the best- and second-best performing models with the \"uniform\" scheme, and K \\(\\in\\) {3,4,5} with the \"distance\" scheme. Taking into consideration that the task at hand is a two-class problem, we should avoid choosing an even-value for K with the uniform decision-scheme, K = 3 is the optimal choice for this model configuration with either decision-scheme.  </p>"},{"location":"projects/ml/#data-reduction","title":"Data reduction","text":"<p>Another problem is a time- and resource-problem. The KNN classifier needs to compute and store as many distances as training data points for each new data point \\(\\hat{x}\\). The problem makes us question the necessity of including every single training point in the classifier. To tackle this problem, we identify the prototypes, a set of data points such that a 1-NN classifier can perform as accurately as a 1-NN classifier trained by the entire training data set.  </p> <p>Let \\(\\mathbb{X} = \\mathbb{O} \\; \\cup \\mathbb{P} \\; \\cup \\mathbb{A}\\), with \\(\\mathbb{X}\\) training data set, \\(\\mathbb{O}\\) set of outliers, \\(\\mathbb{A}\\) set of absorbed points, and \\(\\mathbb{P}\\) set of prototypes. We identify outliers as follows: For \\(x_i \\in \\mathbb{X}\\),</p> <ol> <li>classify \\(x_i\\) with KNN classifer trained by set $\\hat{\\mathbb{X}} = \\mathbb{X} \\setminus {x_i} $, and compare to the true class label of \\(x_i\\)</li> <li>if \\(x_i\\) is incorrectly classified, \\(x_i\\) is an outlier. </li> </ol> <p>To detect absorbed points and prototype, we do as follows:  </p> <ol> <li>assume \\(\\mathbb{A} = \\mathbb{X}\\) and \\(\\mathbb{P} = \\varnothing\\).</li> <li>initilize with random \\(x_i \\in \\mathbb{A}\\) , \\(\\mathbb{A} = \\mathbb{A} \\setminus \\{x_i\\}\\) and \\(\\mathbb{P} = \\mathbb{P} \\cup \\{x_i\\}\\)</li> <li>assume a helping set $ U = \\mathbb{A}$</li> <li>while \\(U \\neq \\varnothing\\), we do:  <ol> <li>choose \\(x \\in U\\) and classify \\(x\\) using 1-NN classifier trained by \\(\\mathbb{P}\\).</li> <li>if 1-NN predicted incorrectly, then \\(\\mathbb{A} = \\mathbb{A} \\setminus \\{x_i\\}\\) and \\(\\mathbb{P} = \\mathbb{P} \\cup \\{x_i\\}\\), and $ U = \\mathbb{A}$. Else, \\(\\mathbb{U} = \\mathbb{U} \\setminus \\{x_i\\}\\).</li> </ol> </li> </ol> <p>This process of identifying prototypes involves randomly choices for \\(x\\) at step 3, and then step 4. Therefore, running the process multiple times often yields slightly different prototype sets.  The listing below shows how the dataset changes when we perform the data reduction process.</p> Data reduction Original len(dataset):  637 Outliers:  32 Absorbed points found:   586 24 Prototypes found, seed = 0 16 Prototypes found, seed = 100 19 Prototypes found, seed = 200 <p>The data reduction process has removed 32 outliers from the original dataset, using 3-NN classifier, to make a new dataset. From the new dataset, sans outliers, a set of 24 prototypes are identified, with a fixed seed at 0. Other seeds give different Prototype sets. </p> <p></p> <p>The potentially problematic area lies where the two classes slightly blend together. We find the majortity of outliers in the mixture of two classes, with some outliers (x-markers) scattering the outside perimeter of the data sets. We also find the most prototypes (triangles) in the intersection of the classes. These prototypes can map out the separation between the classes, and classify the entire test set, as (almost) accurately as the whole training set. The absorbed points are the remaining o-markers.  </p>"},{"location":"projects/ml/#classify-validation-dataset","title":"Classify Validation dataset","text":"<p>At this point, we have optimized the KNN classifier with respect to its parameter K (K = 3), and with respect to the training dataset. Based on this result, we can create a new 3-NN classifier trained on only the prototype set, by which we predict the class labels for \\(\\textit{val\\textunderscore dataset}\\).  </p> <p></p>"},{"location":"projects/numerical_mathematics/","title":"Numerical Mathematics","text":"<p>GitHub</p> <p>The code for this project can be found here  </p> <p> Report 1: Finding the root </p> <p> Report 2: Interpolation and decomposition  </p> <ul> <li> Part 1: Interpolation </li> <li> Part 2: Decomposition</li> </ul> <p> Report 3: Length along the curve and maxima/minima</p> <ul> <li> Part 1: Length along the curve </li> <li> Part 2: Finding minima and maxima</li> </ul> <p> Report 4: ODE and PDE</p> <ul> <li> Part 1: Predator-prey ODE system </li> <li> Part 2: Wave equation</li> </ul>"},{"location":"projects/numerical_mathematics/#report-1-finding-the-root","title":"Report 1: Finding the root","text":"<ol> <li> <p>Visualize the function with Matlab  <pre><code>x = 0:0.3:20;\ny = log10(x)+cos(x)-1;\nplot(x,y,'r-o')\ntitle('f(x) = log(x)+cos(x)-1'),\nxlabel('value of x')\nylabel('value of y')\ngrid on\n</code></pre></p> </li> <li> <p>Calculate the root using the integrated \"fzero\" function <pre><code>r1_fzero = fzero(f,6);\nr2_fzero = fzero(f,8);\nr3_fzero = fzero(f,10);\n</code></pre></p> </li> <li> <p>Calculate the root using the \"Bisection\" method <pre><code>x1_bisection = 1;\nx2_bisection = 6;\nepsilon = 1e-7;\n\nminTime = Inf;\nif f(x1_bisection)*f(x2_bisection)&gt;0\n    disp('This is not possible')\nelse\n    r_bisection = 1;\n    err = 1;\n    i_bisection = 0;\n    while err &gt; epsilon\n        t_bisection = tic;\n        i_bisection = i_bisection+1;\n   if f(x1_bisection)*f(r_bisection)&lt;0\n       x2_bisection = r_bisection;\n   else\n       x1_bisection = r_bisection;\n   end\n    r_bisection = (x1_bisection + x2_bisection)/2;\n   err = abs(f(r_bisection));\n    end\nend\nt_bisection_end = toc(t_bisection);\n</code></pre></p> </li> <li> <p>Calculate the root using the \"False Position\" method <pre><code>x1_false = 1;\nx2_false = 6;\n\nif f(x1_false)*f(x2_false)&gt;0\n    disp('This is not possible')\nelse\n    r_false = 1;\n    error = abs(f(r_false));\n    i_false = 0;\nwhile error &gt; epsilon\n        t_false = tic;\n        i_false = i_false + 1;\n        if f(x1_false)*f(r_false)&lt;0\n            x2_false = r_false;\n        else\n            x1_false = r_false;\n        end\n        r_false = x1_false-((x2_false - x1_false)*f(x1_false))/(f(x2_false)-f(x1_false));\n   error = abs(f(r_false));\nend\nend\nt_false_end = toc(t_false);\n</code></pre></p> </li> <li> <p>Calculate the root using the \"Secant\" method <pre><code>x0_secant = 2;\nx1_secant = 6;\nx2_secant = 1;\nr_secant = 1;\n\ni_secant = 0;\nwhile r_secant &gt; epsilon\n    t_secant = tic;\n    i_secant = i_secant +1;\n    x2_secant=x1_secant-f(x1_secant).*((x1_secant-x0_secant)/(f(x1_secant)-f(x0_secant)));\n    x0_secant = x1_secant;\n    x1_secant = x2_secant;\n    r_secant = abs(f(x2_secant));\n    s_secant = x2_secant;\nend\nt_secant_end = toc(t_secant);\n</code></pre></p> </li> <li> <p>Calculate the root using the \"Newton-Raphson\" method <pre><code>df =@(x) 1/(x*log(10)) - sin(x);\nx_newton = 6;\n%dif =@(x_newton) diff(f2);\nr_newton = 1;\n\ni_newton = 0;\nwhile r_newton &gt; epsilon\n    t_newton = tic;\n    i_newton = i_newton + 1;\n    x1_newton=x_newton-f(x_newton)/df(x_newton);\n    x_newton = x1_newton;\n    r_newton = abs(f(x1_newton));\n    s_newton = x1_newton;\nend\nt_newton_end = toc(t_newton);\n</code></pre></p> </li> <li> <p>Compare the root finding methods <pre><code>Number of iterations with \"Bisection method\" is:  25\nNumber of iterations with \"False Position method\" is:   6\nNumber of iterations with \"Secant method\" is:   6\nNumber of iterations with \"Newton-Raphson\" is:   5\n</code></pre></p> </li> <li> <p>Compare the root finding methods to the integrated fzero function <pre><code>The diference with \"fzero\" and  \"Bisection method\" is:  0.0000000264\nThe diference with \"fzero\" and \"False Position method\" is:  0.0000000008\nThe diference with \"fzero\" and \"Secant method\" is:  0.00000000000418\nThe diference with \"fzero\" and \"Newton-Raphson method\" is:  0.0000000013\n</code></pre></p> </li> <li> <p>Visualize using a table to compare the results  </p> </li> </ol> Row Root Iterations Diference Time Bisection 5.0166 25 2.6443e-08 8.6000e-04 False Position 5.0166 6 8.0643e-10 8.1200e-04 Secant 5.0166 6 4.1771e-12 8.1700e-04 Newton-Raphson 5.0166 5 1.2582e-09 0.0018"},{"location":"projects/numerical_mathematics/#report-2-interpolation-and-decomposition","title":"Report 2: Interpolation and decomposition","text":"<ol> <li> <p>Bilinear interpolation  </p> <ol> <li> <p>We using the \u2019bilinear interpolation method\u2019 to interpolate points in the domain \\([\u22125, 5]\u00d7[\u22125, 5]\\) using the function \\(f(x, y) = x + cos(y) \u2212 1\\) respectively: \\(f(\u22125, \u22125), f(5, \u22125), f(5, 5) and f(\u22125, 5)\\). <pre><code>function P = bilinear(x,y)\n    x1 = -5;  x2 = 5;\n    y1 = -5;  y2 = 5;\n\n    f =@(x,y) x+cos(y)-1;\n\n    Q11 = f(x1,y1);  Q12 = f(x1,y2);\n    Q21 = f(x2,y1);  Q22 = f(x2,y2);\n\n    R1 = ((x2 - x)/(x2 - x1))*Q11 + ((x - x1)/(x2 - x1))*Q21;\n    R2 = ((x2 - x)/(x2 - x1))*Q12 + ((x - x1)/(x2 - x1))*Q22;\n\n    P = ((y2 - y)/(y2 - y1))*R1 + ((y - y1)/(y2 - y1))*R2;\nend\n</code></pre> For the 1 point:  x1: 3.15    x2: -3.42    y: 2.43 For the 2 point:  x1: 4.06    x2: 4.71    y: 3.34 For the 3 point:  x1: -3.73    x2: 4.57    y: -4.45 For the 4 point:  x1: 4.13    x2: -0.15    y: 3.42 For the 5 point:  x1: 1.32    x2: 3.00    y: 0.61 For the 6 point:  x1: -4.02    x2: -3.58    y: -4.74 For the 7 point:  x1: -2.22    x2: -0.78    y: -2.93 For the 8 point:  x1: 0.47    x2: 4.16    y: -0.25 For the 9 point:  x1: 4.58    x2: 2.92    y: 3.86 </p> </li> <li> <p>Interpolate <pre><code>x1 = -5;  x2 = 5;\ny1 = -5;  y2 = 5;\nf=@(x,y) x+cos(y)-1;\nQ11 = f(x1,y1);  Q12 = f(x1,y2);\nQ21 = f(x2,y1);  Q22 = f(x2,y2);\n\na = -5; b = 5;\nrr = (b-a).*rand(10,2) + a;\nr = [(rr(:,1)),(rr(:,2))];\nfor i = 1:9\n    P = bilinear(r(i,1),r(i,2));\n    plot3(r(i,1),r(i,2),P,'or') %Plot the interpolated points\nend\n</code></pre></p> </li> <li> <p>Compare the interpolated values with the exact values given by \\(f(x, y) = x + cos(y) \u2212 1\\) and summarize the results. <pre><code>for i = 1:9\n    fy = f((r(i,1)),(r(i,2)));\n    P = bilinear(r(i,1),r(i,2));\n    result = fy - P;\n    result = abs(result);\nend\n</code></pre> For the \\(1^{st}\\) point: 1.24 For the \\(2^{nd}\\) point: 0.29 For the \\(3^{rd}\\) point: 0.42 For the \\(4^{th}\\) point: 0.71 For the \\(5^{th}\\) point: 1.27 For the \\(6^{th}\\) point: 1.19 For the \\(7^{th}\\) point: 0.43 For the \\(8^{th}\\) point: 0.81 For the \\(9^{th}\\) point: 1.26 </p> </li> <li> <p>Visualizeing the function and the interpolated values. </p> </li> </ol> </li> <li> <p>QR decomposition  </p> <ol> <li>Write a Matlab code to calculate the QR decomposition of a real 5x5 matrix using the Gram\u2013Schmidt process. <pre><code>function [Q,R] =gschmidt(V)\n    [m,n]=size(V);\n    R=zeros(n);\n    R(1,1)=norm(V(:,1));\n    Q(:,1)=V(:,1)/R(1,1);\n    for k =2:n\n        R(1:k-1,k)=Q(:,1:k-1)'*V(:,k);\n        Q(:,k)=V(:,k)-Q(:,1:k-1)*R(1:k-1,k);\n        R(k,k)=norm(Q(:,k))\n        Q(:,k)=Q(:,k)/R(k,k)\n    end\nend\n</code></pre></li> <li>Define an orthogonal matrix Q and an upper triangular matrix \\(R\\) with \\(A = Q \u00b7 R\\) to test your program. <pre><code>A = rand(5);\n[W,U] = qr(A);\nB_nomral = W*U\n[Q,R] = gschmidt(A);\nB_qr = Q*R\n</code></pre></li> <li>Extend the code to solve the system \\(A\u00b7 x = b\\), where b needs to be set accordingly, and compare the result with the built in Matlab function A \\ b. ```matlab x = rand(5,1);</li> </ol> <p>b=A\\x; inv(A)\\b;</p> </li> </ol>"},{"location":"projects/numerical_mathematics/#report-3-differentiation-and-integration","title":"Report 3: Differentiation and integration","text":"<ol> <li> <p>The length along a curve \\(y = y(x)\\) from \\(x = a\\) to \\(x = b\\) is given by the expression   $$   S = \\int_{x=a}^{b} \\sqrt{1 +  (\\dfrac{dx}{dy})^{2} dx.}   $$   In most cases this expression cannot be determined analytically. Calculating the given expression numerically for an arbitrary smooth function in the interval \\([a, b]\\).   Differentiate the function using the forward difference scheme <pre><code>dFun = @(x) Fun(x+h)-Fun(x-h)/2*h;\n</code></pre>   Define the way to calculate the length along the curve <pre><code>S =@(x) sqrt(1 + dFun(x).^2);\n</code></pre>   Use the function Gauss_quad to integrate numerically <pre><code>GQ = Gauss_quad(S,1,2);\n</code></pre>   Define the derivative of the function   <pre><code>dFun2 = @(x) x.^2/2 - 1./(2*x.^2);\n</code></pre>   Calculate the length of te curve with the derivative <pre><code>S2 =@(x) sqrt(1 + dFun2(x).^2);\n</code></pre>   Integrated function   <pre><code>inte = integral(S2,1,2);\n</code></pre>   Now we calculate the difference between our function and the integrated one <pre><code>difference = GQ - inte;\n</code></pre> The difference between the integrated function and our iteration is: 3.339997e-02 or 97.70 percent</p> </li> <li> <p>Finding the local extreme values of a function The extreme values of a function \\(y = y(x)\\) are given by \\(y_0(x) = 0\\). However, it has to be checked through the second derivative, if an extreme value is a minima or a maxima. Differentiate the function using the forward difference scheme <pre><code>dy_hd = @(x) y(x+h)-2*y(x)+y(x-h)./h^2;\n</code></pre> Then we check if we have a minima or maxima: <pre><code>minima = [];\nmaxima = [];\nfor i = 1:length(variable)\n\n    if dy_hd(variable(i))&gt;0\n        sol = variable(i);\n        minima =[minima,sol];\n    end\n    if dy_hd(variable(i))&lt;0\n        sol1 = variable(i);\n        maxima = [maxima,sol1];\n    end\nend\n</code></pre> </p> </li> </ol>"},{"location":"projects/sunspot_detection/","title":"Detecting sunspots from a picture of the sun","text":"<p>This is a fun project I started on a weekend. The idea is simple: Get an image and mark the sunspos with a green square.</p> <p>For this task I noticed online that cv2 was the best option to work with picture files.</p> <p>In the begining we load the file using the integreted function of cv2. After that we visualize it. Then the fun part starts. using the function in line 20  we threshold the image so that only the sunspots remain.</p> <p>After the threshholding we have to convert the data into something we can work with. cv2 has a fucntion for this that converts the data into NumPy array. You can see that because we only need the sunspots we have to use the circle equation (line 36) to make sure that we do not detect the sun itself. The next integreted function (line 43) gets us the surface of each sunspot and then we use a for loop to mark each sunspot with a green rectangle (line 33).</p> <p>From the pictures below you can see that from picture 1 that is the input we get the output with he detection.</p> <p></p> <p></p> <pre><code>import os\nimport cv2 # opencv library\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\"\"\"Make the pwd implementation\"\"\"\ncwd = os.getcwd()\nfile = \"\\sunspot1.jpg\"\npath = cwd + file\nimage = cv2.imread(path,0)\n\nimage_1 = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n# perform image thresholding\nret, thresh = cv2.threshold(image, 90, 255, cv2.THRESH_BINARY)\n\n# find contours\ncontours, hierarchy = cv2.findContours(thresh.copy(),cv2.RETR_TREE,cv2.CHAIN_APPROX_NONE)\nvalid_cntrs = []\n\nfor i,cntr in enumerate(contours):\n    x,y,w,h = cv2.boundingRect(cntr)\n    if ((x-249)**2 + (y-249)**2)&lt;= 238**2:\n        valid_cntrs.append(cntr)\n\"\"\"implement image size detection for the contur LINE 36\"\"\"\n\n# count the number of dicovered sunspots\nprint(\"The number of sunspots is: \",len(valid_cntrs))\n\ncontour_sizes = [(cv2.contourArea(contour), contour) for contour in valid_cntrs]\n\nfor i in range(len(valid_cntrs)):\n    x,y,w,h = cv2.boundingRect(contour_sizes[i][1])\n    final = cv2.rectangle(image_1,(x,y),(x+w,y+h),(0,255,0),1)\n\nplt.imshow(final)\nplt.show()\n</code></pre> <p>This code is not perfect and has some problems but its a nice first implementation for a weekend project for a newbie python programer.</p>"}]}